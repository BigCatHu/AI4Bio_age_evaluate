{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24485001-29f0-4fe9-9eb3-0f64075ca537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import time\n",
    "from sklearn.svm import SVR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "import logging\n",
    "# 禁用 lightgbm 的警告信息\n",
    "logging.getLogger('lightgbm').setLevel(logging.ERROR)\n",
    "\n",
    "# 接下来可以继续正常使用 lightgbm\n",
    "\n",
    "disease_mapping = {\n",
    "    'control': 0,\n",
    "    \"Alzheimer's disease\": 1,\n",
    "    \"Graves' disease\": 2,\n",
    "    \"Huntington's disease\": 3,\n",
    "    \"Parkinson's disease\": 4,\n",
    "    'rheumatoid arthritis': 5,\n",
    "    'schizophrenia': 6,\n",
    "    \"Sjogren's syndrome\": 7,\n",
    "    'stroke': 8,\n",
    "    'type 2 diabetes': 9\n",
    "}\n",
    "sample_type_mapping = {'control': 0, 'disease tissue': 1}\n",
    "\n",
    "\n",
    "def load_idmap(idmap_dir):\n",
    "    idmap = pd.read_csv(idmap_dir, sep=',')\n",
    "    age = idmap.age.to_numpy()\n",
    "    age = age.astype(np.float32)\n",
    "    sample_type = idmap.sample_type.replace(sample_type_mapping)\n",
    "    return age, sample_type\n",
    "\n",
    "\n",
    "def load_methylation(methy_dir):\n",
    "    '''\n",
    "    Load methylation data from csv file.\n",
    "\n",
    "    Note: We set nrows=5000 for test.\n",
    "    If you want to use full data, it is recommended to read csv file by chunks \n",
    "      or other methods since the csv file is very large.\n",
    "    Note the memory usage when you read csv file.\n",
    "\n",
    "    We fill nan with 0, you can try other methods.\n",
    "    '''\n",
    "    methylation = pd.read_csv(methy_dir, sep=',', index_col=0, nrows=5000)\n",
    "    methylation.fillna(0, inplace=True)\n",
    "    methylation = methylation.values.T.astype(np.float32)\n",
    "    return methylation\n",
    "\n",
    "\n",
    "def load_methylation_h5(prefix):\n",
    "    '''\n",
    "    Load methylation data from .h5 file. \n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    prefix: 'train' or 'test'\n",
    "    '''\n",
    "    methylation = h5py.File(prefix + '.h5', 'r')['data']\n",
    "    h5py.File(prefix + '.h5', 'r').close()\n",
    "    print(\"测试数据\", len(methylation))\n",
    "    print(len(methylation[:, :5000]))\n",
    "#     return methylation[:, :5000]  # 5000 just for test\n",
    "    return methylation[:, :]  # If you want to use full data, you can use this line.\n",
    "\n",
    "\n",
    "# def train_ml(X_train, y_train):\n",
    "#     model = ElasticNet()\n",
    "#     model.fit(X_train, y_train)\n",
    "#     return model\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "def train_random_forest(X_train, y_train):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def train_gradient_boosting(X_train, y_train):\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_elastic(X_train, y_train):\n",
    "    model = ElasticNet()\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def train_xgboost(X_train, y_train):\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def train_adaboost(X_train, y_train):\n",
    "    model = AdaBoostRegressor()\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def train_lgbm(X_train, y_train):\n",
    "    model = lgb.LGBMRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "######################\n",
    "\n",
    "def train_lgbm(X_train, y_train):\n",
    "    # 设置模型参数\n",
    "    params = {\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 300,\n",
    "        'subsample': 1.0,\n",
    "        'colsample_bytree': 1.0,\n",
    "        'objective': 'regression',\n",
    "        'metric': 'l1'\n",
    "    }\n",
    "\n",
    "    # 创建 LGBMRegressor 模型\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "    # 拟合模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     # 自定义损失函数的计算逻辑（平方损失）\n",
    "#     loss = np.square(y_true - y_pred).mean()\n",
    "#     return loss\n",
    "\n",
    "# def train_lgbm(X_train, y_train, num_iterations):\n",
    "#     params = {\n",
    "#         'objective': 'regression',  # 设置回归任务的目标函数类型\n",
    "#         'metric': 'l2',  # 使用默认的均方差作为评估指标\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         # 在这里设置其他参数\n",
    "#     }\n",
    "    \n",
    "#     train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    \n",
    "#     model = lgb.train(params, train_data, num_boost_round=num_iterations,\n",
    "#                       valid_sets=[train_data], valid_names=['train'])\n",
    "    \n",
    "#     # 获取训练过程中的损失值\n",
    "#     eval_results = model.evals_result_\n",
    "#     train_loss = eval_results['train']['l2']\n",
    "    \n",
    "#     # 绘制损失曲线\n",
    "#     plt.plot(train_loss, label='Train Loss')\n",
    "#     plt.xlabel('Iterations')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "#     return model\n",
    "\n",
    "\n",
    "##############################################\n",
    "# def train_loss(y_true, y_pred, sample_type):\n",
    "#     mae_control = np.mean(np.abs(y_true[sample_type == 0] - y_pred[sample_type == 0])) # 对照组样本的平均绝对误差\n",
    "\n",
    "#     case_true = y_true[sample_type == 1]\n",
    "#     case_pred = y_pred[sample_type == 1]\n",
    "#     above = np.where(case_pred >= case_true)\n",
    "#     below = np.where(case_pred < case_true)\n",
    "\n",
    "#     ae_above = np.sum(np.abs(case_true[above] - case_pred[above])) / 2\n",
    "#     ae_below = np.sum(np.abs(case_true[below] - case_pred[below]))\n",
    "#     mae_case = (ae_above + ae_below) / len(case_true)\n",
    "#     mae = np.mean([mae_control, mae_case])\n",
    "#     return mae\n",
    "\n",
    "\n",
    "def train_ml(X_train, y_train, model_type):\n",
    "    if model_type == 'lstm':\n",
    "        model = train_lstm(X_train, y_train)\n",
    "    elif model_type == 'xgboost':\n",
    "        model = train_xgboost(X_train, y_train)\n",
    "    elif model_type == 'adaboost':\n",
    "        model = train_adaboost(X_train, y_train)\n",
    "    elif model_type == 'lgbm':\n",
    "        model = train_lgbm(X_train, y_train)\n",
    "    elif model_type == \"elastic\":\n",
    "        model = train_elastic(X_train, y_train)\n",
    "    elif model_type == 'random_forest':  # 添加随机森林模型类型\n",
    "        model = train_random_forest(X_train, y_train)\n",
    "    elif model_type == 'gradient_boosting':  # 添加提升树模型类型\n",
    "        model = train_gradient_boosting(X_train, y_train)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "def evaluate_ml(y_true, y_pred, sample_type):\n",
    "    '''\n",
    "    This function is used to evaluate the performance of the model. \n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    y_true: true age\n",
    "    y_pred: predicted age\n",
    "    sample_type: sample type, 0 for control, 1 for case\n",
    "    \n",
    "    Return:\n",
    "    ------------\n",
    "    mae: mean absolute error.\n",
    "    mae_control: mean absolute error of control samples. 对照组样本的平均绝对误差\n",
    "    mae_case: mean absolute error of case samples.  病例组样本的平均绝对误差\n",
    "\n",
    "    We use MAE to evaluate the performance.\n",
    "    Please refer to evaluation section in the the official website for more details.\n",
    "    '''\n",
    "    mae_control = np.mean(np.abs(y_true[sample_type == 0] - y_pred[sample_type == 0])) # 对照组样本的平均绝对误差\n",
    "\n",
    "    case_true = y_true[sample_type == 1]\n",
    "    case_pred = y_pred[sample_type == 1]\n",
    "    \n",
    "    #################\n",
    "#     def adjust_predicted_age(case_true,case_pred):\n",
    "#         case_pred[case_pred < case_true] += 0.3\n",
    "#         return case_pred\n",
    "#33333333333333333333333333\n",
    "    print(type(case_pred))\n",
    "    print(case_pred.shape)\n",
    "\n",
    "#     case_pred = adjust_predicted_age(case_true,case_pred)\n",
    "    above = np.where(case_pred >= case_true)\n",
    "    below = np.where(case_pred < case_true)\n",
    "\n",
    "    ae_above = np.sum(np.abs(case_true[above] - case_pred[above])) / 2\n",
    "    ae_below = np.sum(np.abs(case_true[below] - case_pred[below]))\n",
    "    mae_case = (ae_above + ae_below) / len(case_true)\n",
    "\n",
    "    mae = np.mean([mae_control, mae_case])\n",
    "    return mae, mae_control, mae_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b0bad-fdb3-45ad-92e5-a4bec4a1d8f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据 8233\n",
      "8233\n",
      "测试数据 2063\n",
      "2063\n",
      "Load data done\n",
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# idmap_train_dir = '/trainmap.csv'\n",
    "idmap_train_dir = '/mnt/Bioage/updated_trainmap.csv'\n",
    "\n",
    "methy_train_dir = '/traindata.csv'\n",
    "idmap_test_dir = '/mnt/Bioage/ai4bio_testset_final/testmap.csv'\n",
    "methy_test_dir = '/mnt/Bioage/ai4bio_testset_final/testdata.csv'\n",
    "\n",
    "age, sample_type = load_idmap(idmap_train_dir)\n",
    "\n",
    "# Note: 'traindata.csv' is about 57GB, 'testdata.csv' is about 15GB.\n",
    "# If you want to use h5 file,  you must run data_h5.py first to generate .h5 file.\n",
    "# 'train.h5' is about 15GB, 'test.h5' is about 3.8GB.\n",
    "# However, the memory usage is still large when you load .h5 file.\n",
    "# Using this code directly on the free server provided by Tianchi will still\n",
    "# result in insufficient memory when training ElasticNet on the full dataset.\n",
    "\n",
    "use_h5 = True\n",
    "if use_h5:\n",
    "    methylation = load_methylation_h5('train')\n",
    "    methylation_test = load_methylation_h5('test')\n",
    "else:\n",
    "    methylation = load_methylation(methy_train_dir)\n",
    "    methylation_test = load_methylation(methy_test_dir)\n",
    "print('Load data done')\n",
    "\n",
    "indices = np.arange(len(age))\n",
    "[indices_train, indices_valid, age_train, age_valid] = train_test_split(indices, age, test_size=0.3, shuffle=True) # random_state=42,\n",
    "\n",
    "methylation_train, methylation_valid = methylation[indices_train], methylation[indices_valid]\n",
    "\n",
    "sample_type_train, sample_type_valid = sample_type[indices_train], sample_type[indices_valid]\n",
    "\n",
    "feature_size = methylation_train.shape[1]\n",
    "del methylation\n",
    "print('Start training...')\n",
    "start = time.time()\n",
    "\n",
    "model_type = 'lgbm'  # 可选择 'lstm', 'xgboost', 'adaboost', 'lgbm' 'elastic' 'random_forest   gradient_boosting\n",
    "\n",
    "\n",
    "pred_model = train_ml(methylation_train, age_train, model_type)\n",
    "\n",
    "# # 获取特征重要性得分\n",
    "# feature_importance = pred_model.feature_importances_\n",
    "\n",
    "# # 创建特征重要性的 DataFrame\n",
    "# importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})\n",
    "\n",
    "# # 按重要性降序排序\n",
    "# importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# # 打印特征重要性排名\n",
    "# print(importance_df)\n",
    "\n",
    "# pred_model = train_ml(methylation_train, age_train)\n",
    "print(f'Training time: {time.time() - start}s')\n",
    "\n",
    "age_valid_pred = pred_model.predict(methylation_valid)\n",
    "mae = evaluate_ml(age_valid, age_valid_pred, sample_type_valid)\n",
    "print(f'Validation MAE: {mae}')\n",
    "\n",
    "age_pred = pred_model.predict(methylation_test)\n",
    "\n",
    "age_pred[age_pred < 0] = 0  \n",
    "# naive post-processing to ensure age >= 0\n",
    "\n",
    "age_pred = np.around(age_pred, decimals=2)\n",
    "age_pred = ['%.2f' % i for i in age_pred]\n",
    "sample_id = pd.read_csv(idmap_test_dir, sep=',').sample_id\n",
    "# Note: sample_id in submission should be the same as the order in testmap.csv.\n",
    "# We do not provide the matching producdure for disordered sample_id in evaluation.\n",
    "\n",
    "submission = pd.DataFrame({'sample_id': sample_id, 'age': age_pred})\n",
    "submission_file = 'submit2.txt'\n",
    "submission.to_csv(submission_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b56292d3-9b1b-48eb-8029-ecd755c3208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "xgboost\n",
    "Training time: 11.390065670013428s\n",
    "Validation MAE: (4.891016764822686, 4.7887278, 4.993305769330431)\n",
    "\n",
    "Training time: 1117.8519587516785s\n",
    "Validation MAE: (3.3071934799655844, 3.0595877, 3.5547992428748945)\n",
    "\n",
    "adaboost:\n",
    "Training time: 201.52103209495544s\n",
    "Validation MAE: (9.747131501148274, 10.778602479313903, 8.715660522982644)\n",
    "\n",
    "lgbm:\n",
    "Training time: 7.726862668991089s\n",
    "Validation MAE: (4.622818994742742, 4.831269383461401, 4.414368606024082)\n",
    "LGBMRegressor()\n",
    "Training time: 1762.3586609363556s\n",
    "Validation MAE: (3.018452869747649, 2.943667307732819, 3.0932384317624786)\n",
    "\n",
    "Training time: 1624.514690876007s\n",
    "Validation MAE: (2.9165765886611785, 3.0095042118883777, 2.823648965433979)\n",
    "\n",
    "elastic:\n",
    "Training time: 10.774928569793701s\n",
    "Validation MAE: (8.467154096179167, 9.084146, 7.850162646398862)\n",
    "Training time: 1290.4735975265503s\n",
    "Validation MAE: (4.642511094536491, 5.021964, 4.263058115891831)\n",
    "\n",
    "gradient_boosting:\n",
    "Training time: 481.118070602417s\n",
    "Validation MAE: (6.093369791369678, 6.471409458326512, 5.715330124412843)\n",
    "\n",
    "random_forest:\n",
    "Training time: 1316.468659877777s\n",
    "Validation MAE: (5.396996449669084, 5.10325217586691, 5.690740723471257)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82388fef-dabf-44a6-9dbe-6fca1f0bfc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validation MAE: (3.6879100562352765, 3.5677478235213966, 3.808072288949156)\n",
    "(555,)\n",
    "\n",
    "Validation MAE: (3.575001326467916, 3.3654093121172552, 3.7845933408185766)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94315cd-64d8-4b05-9049-54fce6d374d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 可视化验证集的实际年龄和预测年龄对比\n",
    "plt.scatter(age_valid, age_valid_pred, c=sample_type_valid, cmap='viridis')\n",
    "plt.plot([0, 120], [0, 120], 'r--')  # 理想情况下的对角线\n",
    "plt.xlabel('Actual Age')\n",
    "plt.ylabel('Predicted Age')\n",
    "plt.title('Validation Set: Actual vs Predicted Age')\n",
    "plt.colorbar(label='Sample Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61a164-77bb-4905-9b1f-4db24ca3fc09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d956d888-09d0-40cd-9173-2dd867293d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5763, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(methylation_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e8648-3ce3-4430-96a9-4c742e901934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "\n",
    "def savefile(methy_dir, chunk_size, name):\n",
    "    df_chunks = pd.read_csv(methy_dir,\n",
    "                            sep=',',\n",
    "                            index_col=0,\n",
    "                            chunksize=chunk_size)\n",
    "\n",
    "    with h5py.File(name, 'w') as file:\n",
    "        total_cols = 0\n",
    "        for i, chunk in enumerate(df_chunks):\n",
    "            chunk = chunk.transpose()\n",
    "            chunk = chunk.fillna(0)\n",
    "            # fill nan with 0, you can try other methods\n",
    "            data_array = chunk.to_numpy()\n",
    "            chunk_cols = data_array.shape[1]\n",
    "            if i == 0:\n",
    "                samples_num = data_array.shape[0]\n",
    "                dataset = file.create_dataset('data',\n",
    "                                              shape=data_array.shape,\n",
    "                                              maxshape=(samples_num, None))\n",
    "\n",
    "            dataset.resize((dataset.shape[0], total_cols + chunk_cols))\n",
    "\n",
    "            dataset[:, total_cols:total_cols + chunk_cols] = data_array\n",
    "\n",
    "            total_cols += chunk_cols  # Update total_cols within the loop\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "chunk_size = 5000\n",
    "\n",
    "methy_train_dir = '/traindata.csv'\n",
    "savefile(methy_train_dir, chunk_size, 'train.h5')\n",
    "print('transform traindata over')\n",
    "\n",
    "methy_test_dir = '/mnt/Bioage/ai4bio_testset_final/testdata.csv'\n",
    "savefile(methy_test_dir, chunk_size, 'test.h5')\n",
    "print('transform testdata over')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
